---
title: "R Notebook"
output: html_notebook
---

#### Loading Libraries
```{r Libraries, warning=FALSE, message=FALSE}
library(dplyr)
library(tree)
library(randomForest)
library(car)
library(glmnet)
library(caret)
library(gbm)
```

#### Loading and Cleaning Data
```{r Dataset, warning=FALSE, message=FALSE}
original <- read.csv("train.csv")
IDColumns <- read.csv("test.csv")
IDColumns <- IDColumns$Id
# Removing Agreed Upon Variables - Due to proportions being over 95% of one group
cleaned <- original %>% select(., -Id, -Street, -Alley, -Utilities, -LandSlope, -Condition2, -RoofMatl,
                               -BsmtFinSF2, -Heating, -LowQualFinSF, -Functional, -X3SsnPorch, 
                               -PoolArea, -PoolQC, -MiscFeature, -MiscVal)

#--Fixing Some NA's and Nuances Manually--#
cleaned[is.na(cleaned$Electrical),"Electrical"] <- "SBrkr"
cleaned[333, "BsmtFinType2"] <- "GLQ"
levels(cleaned$MSZoning) <- c("C", "FV", "RH", "RL", "RM")

cleaned <- cleaned %>% group_by(., Neighborhood) %>% 
    mutate(., LotFrontage = replace(LotFrontage, is.na(LotFrontage), median(LotFrontage, na.rm = T))) %>%
    ungroup()

cleaned$GarageYrBlt <- factor(ifelse(is.na(cleaned$GarageYrBlt), "No", "Yes"))
cleaned$MasVnrArea[is.na(cleaned$MasVnrArea)] <- 0

#--Changing NA's to Absent--#
cleaned$Fence <- factor(ifelse(is.na(cleaned$Fence), "No", "Yes"))
cleaned$GarageCond <- factor(cleaned$GarageCond, exclude = NULL, levels = c("Ex", "Fa", "Gd", "Po", "TA", NA),
                             labels = c("Ex", "Fa", "Gd", "Po", "TA", "Absent"))
cleaned$GarageQual <- factor(cleaned$GarageQual, exclude = NULL, levels = c("Ex", "Fa", "Gd", "Po", "TA", NA),
                             labels = c("Ex", "Fa", "Gd", "Po", "TA", "Absent"))
cleaned$GarageFinish <- factor(cleaned$GarageFinish, exclude = NULL, levels = c("Fin", "RFn", "Unf", NA),
                             labels = c("Fin", "RFn", "Unf", "Absent"))
cleaned$GarageType <- factor(cleaned$GarageType, exclude = NULL, levels = c("2Types", "Attchd", "Basment", "BuiltIn",
                                                                            "CarPort", "Detchd", NA),
                             labels = c("2Types", "Attchd", "Basment", "BuiltIn", "CarPort", "Detchd", "Absent"))
cleaned$FireplaceQu <- factor(cleaned$FireplaceQu, exclude = NULL, levels = c("Ex", "Fa", "Gd", "Po", "TA", NA),
                             labels = c("Ex", "Fa", "Gd", "Po", "TA", "Absent"))
cleaned$BsmtCond <- factor(cleaned$BsmtCond, exclude = NULL, levels = c("Fa", "Gd", "Po", "TA", NA),
                             labels = c("Fa", "Gd", "Po", "TA", "Absent"))
cleaned$BsmtExposure <- factor(cleaned$BsmtExposure, exclude = NULL, levels = c("Av", "Gd", "Mn", "No", NA),
                               labels = c("Av", "Gd", "Mn", "No", "Absent"))
cleaned$MasVnrType <- factor(cleaned$MasVnrType, exclude = NULL, levels = c("BrkCmn", "BrkFace", "None", "Stone", NA),
                             labels = c("BrkCmn", "BrkFace", "None", "Stone", "Absent"))
cleaned$BsmtQual <- factor(cleaned$BsmtQual, exclude = NULL, levels = c("Ex", "Fa", "Gd", "TA", NA), 
                           labels = c("Ex", "Fa", "Gd", "TA", "Absent"))
cleaned$BsmtFinType1 <- factor(cleaned$BsmtFinType1, exclude = NULL, levels = c("ALQ", "BLQ", "GLQ", "LwQ", "Rec",
                                                                                "Unf", NA), 
                               labels = c("ALQ", "BLQ", "GLQ", "LwQ", "Rec", "Unf", "Absent"))
cleaned$BsmtFinType2 <- factor(cleaned$BsmtFinType2, exclude = NULL, levels = c("ALQ", "BLQ", "GLQ", "LwQ", "Rec",
                                                                                "Unf", NA), 
                               labels = c("ALQ", "BLQ", "GLQ", "LwQ", "Rec", "Unf", "Absent"))

#--Making Nones to Absent--#
cleaned <- cleaned %>% 
    mutate(., MasVnrType = as.character(MasVnrType)) %>% 
    mutate(., MasVnrType = ifelse(MasVnrType == "None", "Absent", MasVnrType))

cleaned$MasVnrType <- factor(cleaned$MasVnrType)


#--Log Transforms on Numerical--#
cleaned$LotFrontage <- log(cleaned$LotFrontage + 1)
cleaned$LotArea <- log(cleaned$LotArea + 1)
cleaned$MasVnrArea <- log(cleaned$MasVnrArea + 1)
cleaned$BsmtFinSF1 <- log(cleaned$BsmtFinSF1 + 1)
cleaned$BsmtUnfSF <- log(cleaned$BsmtUnfSF + 1)
cleaned$X1stFlrSF <- log(cleaned$X1stFlrSF + 1)
cleaned$GrLivArea <- log(cleaned$GrLivArea + 1)
cleaned$WoodDeckSF <- log(cleaned$WoodDeckSF + 1)
cleaned$OpenPorchSF <- log(cleaned$OpenPorchSF + 1)
cleaned$EnclosedPorch<- log(cleaned$EnclosedPorch+ 1) 
cleaned$ScreenPorch <- log(cleaned$ScreenPorch + 1)
cleaned$GarageArea <- log(cleaned$GarageArea + 1)
cleaned$X2ndFlrSF <- log(cleaned$X2ndFlrSF + 1)
cleaned$TotalBsmtSF <- log(cleaned$TotalBsmtSF + 1)
cleaned$SalePrice <- log(cleaned$SalePrice + 1)

#---Fixing into Categories---#
cleaned <- as.data.frame(cleaned)
cleaned$MSSubClass <- factor(cleaned$MSSubClass)
cleaned$OverallQual <- factor(cleaned$OverallQual)
cleaned$OverallCond <- factor(cleaned$OverallCond)
cleaned$BsmtFullBath <- factor(cleaned$BsmtFullBath)
cleaned$BsmtHalfBath <- factor(cleaned$BsmtHalfBath)
cleaned$FullBath <- factor(cleaned$FullBath)
cleaned$HalfBath <- factor(cleaned$HalfBath)
cleaned$BedroomAbvGr <- factor(cleaned$BedroomAbvGr)
cleaned$KitchenAbvGr <- factor(cleaned$KitchenAbvGr)
cleaned$TotRmsAbvGrd <- factor(cleaned$TotRmsAbvGrd)
cleaned$Fireplaces <- factor(cleaned$Fireplaces)
cleaned$GarageCars <- factor(cleaned$GarageCars)
# colnames(cleaned[ , colSums(is.na(cleaned)) > 0])
# summary(cleaned)

write.csv(cleaned, file = "cleanedData2.csv")
```

#### Splitting the Data
```{r Splitting}
set.seed(0)
train.indxs <- sample(1:nrow(cleaned), 0.8*nrow(cleaned))
test.indxs <- -train.indxs

training <- cleaned[train.indxs, ]
testing <- cleaned[test.indxs, ]
```

#### Random Forest Model
```{r Running RandomForest}
set.seed(0)
oob.err <- numeric(64)
for(mtry in 1:64){
    fit <- randomForest(SalePrice ~ ., data = training, mtry = mtry)
    oob.err[mtry] = fit$mse[500]
    cat("We're performing iteration", mtry, "\n")
}

plot(1:64, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")


bstForest <- randomForest(SalePrice ~ ., mtry = 25, 
                          data = training, ntree = 500)
bstForest


varImpPlot(bstForest)
yHat <- predict(bstForest, newdata = testing)
yOrig <- testing$SalePrice

mean((yHat - yOrig)^2)


# PredictedSalePrice <- exp(yHat) + 1
# PredictedSalePrice <- data.frame(ID = IDColumns, SalePrice = PredictedSalePrice)
# write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv")
```
The number of trees at each split is 500
The number of variables at each split is 25.
The mean of squared residuals is 0.01897412.
The percentage of variation explained is 88.48.


#### Running a Linear Regression Model
```{r Regression}
first.model <- lm(SalePrice ~ ., data = training)
summary(first.model)

# model.empty <- lm(SalePrice ~ 1, data = training)
# model.full <- lm(SalePrice ~ ., data = training)
# 
# scope = list(lower = formula(model.empty), upper = formula(model.full))
# bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)

x <- model.matrix(SalePrice ~ ., data = training)[ , -1]
y <- training$SalePrice

x.test <- model.matrix(SalePrice ~., data = testing)[, -1]
y.test <- testing$SalePrice

grid <- 10^seq(5, -2, length = 100)
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)

lasso.pred <- predict(lin.model.lasso, s = 5, newx = x.test)
mean((lasso.pred - y.test)^2)


#---Some Cross Validation---#
set.seed(0)
train_lasso_control <- trainControl(method = 'cv', number = 10)
tune.lasso.grid <- expand.grid(lambda = grid, alpha = seq(0,1, length = 100))
ridge.lasso.caret <- train(x, y, method =  'glmnet', 
                           trControl = train_lasso_control, tuneGrid = tune.lasso.grid)

plot(ridge.lasso.caret, xTrans = log)
pred.lasso <- predict.train(ridge.lasso.caret, newdata = x.test)
mean((pred.lasso - y.test)^2)

```

#### Runnning Gradient Boosting Machine
```{r GBM}

```




# Running the Models with Different Data
```{r}
final.original <- read.csv("train_clean.csv")
final.test <- read.csv("test_clean.csv")

summary(final.original)
summary(final.test)

final.original$MSSubClass <- factor(final.original$MSSubClass)
final.original$SalePrice <- log(final.original$SalePrice + 1)
final.original$GarageArea <- log(final.original$GarageArea + 1)
final.original$X2ndFlrSF <- log(final.original$X2ndFlrSF + 1)
final.original$TotalBsmtSF <- log(final.original$TotalBsmtSF + 1)

final.test$MSSubClass <- factor(final.test$MSSubClass)
final.test$GarageArea <- log(final.test$GarageArea + 1)
final.test$X2ndFlrSF <- log(final.test$X2ndFlrSF + 1)
final.test$TotalBsmtSF <- log(final.test$TotalBsmtSF + 1)


set.seed(0)
oob.err <- numeric(30)
for(mtry in 1:30){
    fit <- randomForest(SalePrice ~ ., data = training, mtry = mtry)
    oob.err[mtry] = fit$mse[500]
    cat("We're performing iteration", mtry, "\n")
}

plot(1:30, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")

final.bstForest <- randomForest(SalePrice ~ ., mtry = 25, data = final.original, ntree = 500)
final.bstForest




varImpPlot(final.bstForest)
final.yHat <- predict(final.bstForest, newdata = final.test)


PredictedSalePrice <- exp(final.yHat) + 1
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE, row.names = FALSE)






#------Fixing the factors that arent present in the training set -----#
for (name in colnames(final.original)) {
    if (class(final.original[[name]]) == "factor")
        print(name)
        print(all(levels(final.original[[name]]) == levels(final.test[[name]])))
        levels(final.test[[name]]) = levels(final.original[[name]])
        print("-------------------")
}
#------------------------------------------------------------------------#

#############################################################################
#
#  Lasso Regularization with GLmnet
#
############################################################################

x.final <- model.matrix(SalePrice ~ ., data = final.original)[ , -1]
y.final <- final.original$SalePrice

x.final.test <- model.matrix(~., data = final.test)[, -1]
y.final.test <- final.test$SalePrice

#---Some Cross Validation---#
set.seed(0)
train_lasso_control <- trainControl(method = 'cv', number = 10)
tune.lasso.grid <- expand.grid(lambda = grid, alpha = seq(0,1, length = 100))
final.ridge.lasso.caret <- train(x.final, y.final, method =  'glmnet', 
                           trControl = train_lasso_control, tuneGrid = tune.lasso.grid)

final.pred.lasso <- predict.train(final.ridge.lasso.caret, newx = x.final.test)


PredictedSalePrice2 <- exp(final.pred.lasso) + 1
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2[-1460])
write.csv(x = PredictedSalePrice2, "KaggleSubmit2.csv", col.names = FALSE, row.names = FALSE)

length(final.pred.lasso)
```