---
title: "Untitled"
author: "Aaron Owen"
date: "11/6/2017"
output: html_document
---

```{r}
library(randomForest)
```

```{r}
train = read.csv("train_clean.csv")

train$GarageYrBlt = as.factor(train$GarageYrBlt)
train$SalePrice = log(train$SalePrice + 1)
train$MSSubClass = as.factor(train$MSSubClass)
train$MoSold = as.factor(train$MoSold)
train$YrSold = as.factor(train$YrSold)
```

```{r}
set.seed(0)
train_indices = sample(1:nrow(train), 8*nrow(train)/10)

train_train = train[train_indices, ]
train_test = train[-train_indices, ]
```

```{r}
set.seed(0)
oob.err = numeric(64)
for (mtry in 1:64) {
  fit = randomForest(SalePrice ~ ., data = train_train, mtry = mtry)
  oob.err[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err) # --> 24 variables
min(oob.err) # --> 0.0188857

rf_best1 = randomForest(SalePrice ~ ., data = train_train, mtry = 24, importance = T)

yhat1 = predict(rf_best1, newdata = train_test)
sqrt(mean((yhat1 - train_test$SalePrice)^2)) # --> 0.1460752

```

```{r}
train2 = train

train$GarageYrBlt = as.numeric(train$GarageYrBlt)
train$MSSubClass = as.numeric(train$MSSubClass)
train$MoSold = as.numeric(train$MoSold)
train$YrSold = as.numeric(train$YrSold)
```

```{r}
set.seed(0)
train_indices = sample(1:nrow(train2), 8*nrow(train2)/10)

train_train2 = train2[train_indices, ]
train_test2 = train2[-train_indices, ]
```

```{r}
set.seed(0)
oob.err2 = numeric(64)
for (mtry in 1:64) {
  fit = randomForest(SalePrice ~ ., data = train_train2, mtry = mtry)
  oob.err2[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err2) # --> 24 variables
min(oob.err2) # --> 0.0188857

rf_best2 = randomForest(SalePrice ~ ., data = train_train2, mtry = 24, importance = T)

yhat2 = predict(rf_best2, newdata = train_test2)
sqrt(mean((yhat2 - train_test2$SalePrice)^2)) # --> 0.1461576
```

```{r}
library(dplyr)
train3 = train

train3 = train3 %>% select(-c(LandContour, Condition1, ExterCond, BsmtCond, BsmtFinType2, CentralAir,
                              Electrical, GarageQual, GarageCond, PavedDrive, EnclosedPorch, ScreenPorch,
                              SaleType))

train3_train = train3[train_indices, ]
train3_test = train3[-train_indices, ]

set.seed(0)
oob.err3 = numeric(51)
for (mtry in 1:51) {
  fit = randomForest(SalePrice ~ ., data = train3_train, mtry = mtry)
  oob.err3[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err3) # --> 18 variables
min(oob.err3) # --> 0.01800853

set.seed(0)
rf_best3 = randomForest(SalePrice ~ ., data = train3_train, mtry = 18, importance = T)

yhat3 = predict(rf_best3, newdata = train3_test)
sqrt(mean((yhat3 - train3_test$SalePrice)^2)) # --> 0.1446114
```

```{r}
train4 = read.csv("train_clean.csv")
train4$SalePrice = log(train4$SalePrice + 1)
```

```{r}
train4_train = train4[train_indices, ]
train4_test = train4[-train_indices, ]

set.seed(0)
oob.err4 = numeric(30)
for (mtry in 1:30) {
  fit = randomForest(SalePrice ~ ., data = train4_train, mtry = mtry)
  oob.err4[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err4) # --> 20 variables
min(oob.err4) # --> 0.0179945

set.seed(0)
rf_best4 = randomForest(SalePrice ~ ., data = train4_train, mtry = 20, importance = T)

yhat4 = predict(rf_best4, newdata = train4_test)
sqrt(mean((yhat4 - train4_test$SalePrice)^2)) # --> 0.141765

varImpPlot(rf_best4)
sort(importance(rf_best4)[, 1])

rf_best4_all = randomForest(SalePrice ~ ., data = train4, mtry = 20, importance = T)

TEST = read.csv("test_clean.csv")
TEST$SalePrice = NULL

# ----------------------

FINAL = predict(rf_best4, newdata = TEST)
FINAL1 = exp(FINAL - 1)
FINAL2 = exp(FINAL) - 1

summary(FINAL)

for (name in colnames(train4)) {
    if (class(train4[[name]]) == "factor")
        print(name)
        print(all(levels(train4[[name]]) == levels(TEST[[name]])))
        levels(TEST[[name]]) = levels(train4[[name]])
        print("-------------------")
}

write.csv(FINAL1, "sub1.csv", row.names = F)
write.csv(FINAL2, "sub2.csv", row.names = F)

head(FINAL1)
head(FINAL2)
```

```{r}
train4.2 = train4

train4.2 = train4.2 %>% select()

train4.2_train = train4.2[train_indices, ]
train4.2_test = train4.2[-train_indices, ]

set.seed(0)
oob.err4.2 = numeric(30)
for (mtry in 1:30) {
  fit = randomForest(SalePrice ~ ., data = train4_train, mtry = mtry)
  oob.err4.2[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err4.2) # --> 20 variables
min(oob.err4.2) # --> 0.0179945

set.seed(0)
rf_best4.2 = randomForest(SalePrice ~ ., data = train4.2_train, mtry = 20, importance = T)

yhat4.2 = predict(rf_best4.2, newdata = train4.2_test)
sqrt(mean((yhat4.2 - train4.2_test$SalePrice)^2)) # --> 0.1444745
```

```{r}
train5 = train4

train5 = train5 %>% select(-c(LandContour, Condition1, ExterCond, BsmtCond, BsmtFinType2, CentralAir,
                              Electrical, GarageQual, GarageCond, PavedDrive, EnclosedPorch, ScreenPorch,
                              SaleType))

train5_train = train5[train_indices, ]
train5_test = train5[-train_indices, ]

set.seed(0)
oob.err5 = numeric(30)
for (mtry in 1:30) {
  fit = randomForest(SalePrice ~ ., data = train5_train, mtry = mtry)
  oob.err5[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err5) # --> 18 variables
min(oob.err5) # --> 0.01801212

set.seed(0)
rf_best5 = randomForest(SalePrice ~ ., data = train5_train, mtry = 18, importance = T)

yhat = predict(rf_best5, newdata = train5_test)
sqrt(mean((yhat - train5_test$SalePrice)^2)) # --> 0.02091268
```

                        --------------- train4 is best model -------------
                            ----------scaling numeric categories-------

```{r}
# scale train4 variables
train4.3 = train4

train4.3$MSSubClass = as.factor(train4.3$MSSubClass)

x = c()
cat = data.frame()
for (name in colnames(train4.3)) {
    if (class(train4.3[[name]]) == "factor") {
        x = append(x, name)
    }
}

cat = train4[x]
SalePrice = train4.3$SalePrice

z = c()
for (name in colnames(train4.3)) {
    if (class(train4.3[[name]]) != "factor") {
        z = append(z, name)
    }
}

z = z[-31]
to_scale = train4[z]

length(colnames(cat)) + length(colnames(to_scale))

cat
to_scale

cat = train4 %>% select(MSSubClass, MSZoning, LotShape, LandContour, LotConfig, Neighborhood,
                        Condition1, BldgType, HouseStyle, RoofStyle, Exterior1st, Exterior2nd,
                        MasVnrType, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure,
                        BsmtFinType1, BsmtFinType2, HeatingQC, CentralAir, Electrical, KitchenQual,
                        FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, 
                        PavedDrive, Fence, MoSold, YrSold, SaleType, SaleCondition, SalePrice)
to_scale = train4 %>% select(LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd,
                             MasVnrArea, BsmtFinSF1, BsmtUnfSF, TotalBsmtSF, X1stFlrSF, X2ndFlrSF,
                             GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
                             KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageCars, GarageArea, WoodDeckSF,
                             OpenPorchSF, EnclosedPorch, ScreenPorch)

scaled = as.data.frame(apply(to_scale, 2, scale))

train4_scaled = cbind(cat, scaled)
```

```{r}
train4_scaled_train = train4_scaled[train_indices, ]
train4_scaled_test = train4_scaled[-train_indices, ]

set.seed(0)
oob.err4_scale = numeric(64)
for (mtry in 1:64) {
  fit = randomForest(SalePrice ~ ., data = train4_scaled_train, mtry = mtry)
  oob.err4_scale[mtry] = fit$mse[500] # oob error for each mtry 
  cat("We're performing iteration", mtry, "\n")
}

which.min(oob.err4_scale) # --> 25 variables
min(oob.err4_scale) # --> 0.01781104

set.seed(123)
rf_best4_scaled = randomForest(SalePrice ~ ., data = train4_scaled_train, mtry = 25, importance = T)

yhat4 = predict(rf_best4_scaled, newdata = train4_scaled_test)
sqrt(mean((yhat4 - train4_scaled_test$SalePrice)^2)) # --> 0.1427262
```







                        ---------------------- Boosting --------------------------









```{r}
library(gbm)
set.seed(0)
boost.train = gbm(SalePrice ~ ., data = train4_test,
                   distribution = "gaussian",
                   n.trees = 10000,
                   interaction.depth = 4) # shrinkage default is 0.001

#Inspecting the relative influence.
summary(boost.train)
```

```{r}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.train, newdata = train4_test, n.trees = n.trees)
```

```{r}
berr = with(train4_test, apply((predmat - SalePrice)^2, 2, mean)) 
# berr = mean square error; "boosting error" == berr
plot(n.trees, berr, pch = 16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")

#Include the best OOB error from the random forest.
abline(h = min(oob.err), col = "red")
```

```{r}
set.seed(0)
boost.train2 = gbm(SalePrice ~ ., data = train4_train,
                    distribution = "gaussian",
                    n.trees = 10000,
                    interaction.depth = 4, 
                    shrinkage = 0.049,
                    cv.folds = 5)
predmat2 = predict(boost.train2, newdata = train4_test, n.trees = n.trees)

berr5 = with(train4_test, apply((predmat2 - SalePrice)^2, 2, mean))

plot(n.trees, berr5, pch = 16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")

# ---- interaction depth 4; shrinkage 0.1
sqrt(min(berr1)) # --> 0.1323668
which.min(berr1) # --> 2100

# ---- interaction depth 5; shrinkage 0.1
sqrt(min(berr2)) # --> 0.1347685
which.min(berr2) # --> 600

# ---- interaction depth 6; shrinkage 0.1
sqrt(min(berr3)) # --> 0.1356421
which.min(berr3) # --> 1300

# ---- interaction depth 6; shrinkage 0.1
sqrt(min(berr4)) # --> 0.1382799
which.min(berr4) # --> 1000

# ---- interaction depth 4; shrinkage 0.05
sqrt(min(berr5)) # --> 0.132601
which.min(berr5) # --> 700

library(caret)

gbmGrid = expand.grid(interaction.depth = (4:6), n.trees = seq(500, 5000, by = 500), 
                      shrinkage = c(1, 0.05), n.minobsinnode = 10)
gbmGrid

seq(500, 5000, by = 500)

set.seed(2)
fitControl = trainControl(method = "cv", number = 5, verboseIter = TRUE)

gbmFit <- train(SalePrice ~ ., data = train4_train, method = "gbm", 
                trControl = fitControl, verbose = FALSE, bag.fraction = 0.5, 
                tuneGrid = gbmGrid)

# ----- caret to train

boost_best = train(SalePrice ~ ., data = train4_train, method = "gbm", 
                   verbose = FALSE, bag.friction = 0.5,
                   interaction.depth = 6, n.trees = 500, shrinkage = 0.05)

# ----- using gbm

boost.train.best = gbm(SalePrice ~ ., data = train4_train,
                   distribution = "gaussian",
                   n.trees = 500,
                   interaction.depth = 6,
                   shrinkage = 0.05)

predmat_best = predict(boost.train.best, newdata = train4_test, n.trees = n.trees)

predmat_best

berr_best = with(train4_test, apply((predmat_best - SalePrice)^2, 2, mean))
plot(n.trees, berr5, pch = 16,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")

sqrt(min(berr_best)) # --> 0.0350405


boost.train.best.all = gbm(SalePrice ~ ., data = train4,
                   distribution = "gaussian",
                   n.trees = 500,
                   interaction.depth = 6,
                   shrinkage = 0.05)

FINAL.boost = predict(boost.train.best.all, newdata = TEST, n.trees = 500)
FINAL.boost = exp(FINAL.boost) - 1
FINAL.boost

write.csv(FINAL.boost, "sub3.csv")

x = (FINAL2 + FINAL.boost)/2
write.csv(x, "sub4.csv")
```




