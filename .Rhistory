# plot(1:64, oob.err, pch = 16, type = "b",
#      xlab = "Variables Considered at Each Split",
#      ylab = "OOB Mean Squared Error",
#      main = "Random Forest OOB Error Rates\nby # of Variables")
#
#
fit <- randomForest(SalePrice ~., data = training, mtry = 20)
fit
set.seed(0)
oob.err <- numeric(64)
for(mtry in 1:64){
fit <- randomForest(SalePrice ~ ., data = training, mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:64, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
?randomForest
bstForest <- randomForest(SalePrice ~., data = training, mtry = 20, ntree = 1000)
bstForest
yHat <- predict(bstForest, newdata = testing)
yOrig <- testing$SalePrice
mean((yhat - yOrig)^2)
bstForest <- randomForest(SalePrice ~., data = training, mtry = 20, ntree = 500)
bstForest
yHat <- predict(bstForest, newdata = testing)
yOrig <- testing$SalePrice
mean((yhat - yOrig)^2)
str(testing)
set.seed(0)
oob.err <- numeric(64)
for(mtry in 1:64){
fit <- randomForest(SalePrice ~ ., data = training, mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
varImpPlot(bstForest)
varImpPlot(bstForest)[1:5]
varImpPlot(bstForest)
bstForest <- randomForest(SalePrice ~ OverallQual + Neighborhood + GrLivArea,
data = training, mtry = 20, ntree = 500)
bstForest <- randomForest(SalePrice ~ OverallQual + Neighborhood + GrLivArea,
data = training, ntree = 500)
bstForest
varImpPlot(bstForest)
yHat <- predict(bstForest, newdata = testing)
yOrig <- testing$SalePrice
mean((yhat - yOrig)^2)
testing$SalePrice
training$SalePrice
yHat
yOrig
(yhat - yOrig)
yHat
mean((yHat - yOrig)^2)
bstForest <- randomForest(SalePrice ~ ., mtry = 25,
data = training, ntree = 500)
yHat <- predict(bstForest, newdata = testing)
yOrig <- testing$SalePrice
mean((yHat - yOrig)^2)
bstForest
rm(yhat)
mean((yHat - yOrig)^2)
saturated.model <- lm(SalePrice ~ ., data = training)
summary(saturated.model)
rm(saturated.model)
library(car)
first.model <- lm(SalePrice ~ ., data = training)
vif(first.model)
summary(first.model)
model.empty <- lm(SalePrice ~ 1, data = training)
model.full <- lm(SalePrice ~ ., data = training)
scope = list(lower = formula(model.empty), upper = formula(model.full))
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
summary(bothAIC.empty)
model.reg.2 <- lm(formula = SalePrice ~ OverallQual + GrLivArea + Neighborhood +
BsmtFinSF1 + OverallCond + MSSubClass + GarageCars + YearBuilt +
LotArea + Fireplaces + BsmtExposure + SaleCondition + Condition1 +
MSZoning + BsmtFullBath + FullBath + KitchenQual + TotalBsmtSF +
BsmtCond + ExterCond + ScreenPorch + SaleType + Foundation +
HalfBath + CentralAir + GarageType + Electrical + YearRemodAdd +
TotRmsAbvGrd + WoodDeckSF + MasVnrType + LotConfig + X2ndFlrSF +
GarageCond + Exterior1st + EnclosedPorch + YrSold + BsmtQual +
BsmtUnfSF + BsmtFinType1 + BedroomAbvGr + X1stFlrSF -1, data = training)
summary(model.reg.2)
vif(model.reg.2)
alias( lm(formula = SalePrice ~ OverallQual + GrLivArea + Neighborhood +
BsmtFinSF1 + OverallCond + MSSubClass + GarageCars + YearBuilt +
LotArea + Fireplaces + BsmtExposure + SaleCondition + Condition1 +
MSZoning + BsmtFullBath + FullBath + KitchenQual + TotalBsmtSF +
BsmtCond + ExterCond + ScreenPorch + SaleType + Foundation +
HalfBath + CentralAir + GarageType + Electrical + YearRemodAdd +
TotRmsAbvGrd + WoodDeckSF + MasVnrType + LotConfig + X2ndFlrSF +
GarageCond + Exterior1st + EnclosedPorch + YrSold + BsmtQual +
BsmtUnfSF + BsmtFinType1 + BedroomAbvGr + X1stFlrSF -1, data = training) )
x <- model.matrix(SalePrice ~ ., data = training)[ , -1]
y <- training$SalePrice
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
library(glmnet)
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
grid <- 10^seq(5, -2, length = 100)
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.pred <- predict(lin.model.lasso, s = 5, newx = testing)
grid <- 10^seq(5, -2, length = 100)
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.pred <- predict(lin.model.lasso, s = 5, newx = testing)
x.test <- model.matrix(SalePrice ~., data = testing)[, -1]
y.test <- testing$SalePrice
grid <- 10^seq(5, -2, length = 100)
lin.model.lasso <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.pred <- predict(lin.model.lasso, s = 5, newx = x.test)
y.test <- testing$SalePrice
mean((lasso.lambda5 - y.test)^2)
mean((lasso.pred - y.test)^2)
library(caret)
#---Some Cross Validation---#
set.seed(0)
tune.lasso.grid <- expand.grid(lambda = grid, alpha = seq(0,1, length = 100))
ridge.lasso.caret <- train(training, y, method =  'glmnet', preProc = 'scale',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
train_lasso_control <- trainControl(method = 'cv', number = 10)
tune.lasso.grid <- expand.grid(lambda = grid, alpha = seq(0,1, length = 100))
ridge.lasso.caret <- train(training, y, method =  'glmnet', preProc = 'scale',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
ridge.lasso.caret <- train(training, y, method =  'glmnet',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
ridge.lasso.caret <- train(x, y, method =  'glmnet',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
plot(ridge.lasso.caret, xTrans = log)
pred.lasso <- predict.train(ridge.lasso.caret, newdata = x.test)
mean((pred.lasso - y.test)^2)
final.original <- read.csv("train_clean.csv")
final.test <- read.csv("test_clean.csv")
str(final.original)
final.original$MSSubClass <- factor(final.original$MSSubClass)
final.original$SalePrice <- log(final.original$SalePrice + 1)
final.original$GarageArea <- log(final.original$SalePrice + 1)
final.original$X2ndFlrSF <- log(final.original$X2ndFlrSF + 1)
final.original$TotalBsmtSF <- log(final.original$TotalBsmtSF + 1)
final.test$MSSubClass <- factor(final.test$MSSubClass)
final.test$SalePrice <- log(final.test$SalePrice + 1)
final.original <- read.csv("train_clean.csv")
final.test <- read.csv("test_clean.csv")
str(final.original)
final.original$MSSubClass <- factor(final.original$MSSubClass)
final.original$SalePrice <- log(final.original$SalePrice + 1)
final.original$GarageArea <- log(final.original$SalePrice + 1)
final.original$X2ndFlrSF <- log(final.original$X2ndFlrSF + 1)
final.original$TotalBsmtSF <- log(final.original$TotalBsmtSF + 1)
final.test$MSSubClass <- factor(final.test$MSSubClass)
final.test$GarageArea <- log(final.test$SalePrice + 1)
summary(final.test)
final.original <- read.csv("train_clean.csv")
final.test <- read.csv("test_clean.csv")
str(final.original)
summary(final.test)
final.original$MSSubClass <- factor(final.original$MSSubClass)
final.original$SalePrice <- log(final.original$SalePrice + 1)
final.original$GarageArea <- log(final.original$GarageArea + 1)
final.original$X2ndFlrSF <- log(final.original$X2ndFlrSF + 1)
final.original$TotalBsmtSF <- log(final.original$TotalBsmtSF + 1)
final.test$MSSubClass <- factor(final.test$MSSubClass)
final.test$GarageArea <- log(final.test$GarageArea + 1)
final.test$X2ndFlrSF <- log(final.test$X2ndFlrSF + 1)
final.test$TotalBsmtSF <- log(final.test$TotalBsmtSF + 1)
set.seed(0)
set.seed(0)
oob.err <- numeric(30)
for(mtry in 1:30){
fit <- randomForest(SalePrice ~ ., data = training, mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:30, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
final.bstForest <- randomForest(SalePrice ~ ., mtry = 25, data = final.original, ntree = 500)
final.bstForest
varImpPlot(final.bstForest)
final.yHat <- predict(final.bstForest, newdata = final.test)
final.original <- read.csv("train_clean.csv")
final.test <- read.csv("test_clean.csv")
final.original$MSSubClass <- factor(final.original$MSSubClass)
final.original$SalePrice <- log(final.original$SalePrice + 1)
final.original$GarageArea <- log(final.original$GarageArea + 1)
final.original$X2ndFlrSF <- log(final.original$X2ndFlrSF + 1)
final.original$TotalBsmtSF <- log(final.original$TotalBsmtSF + 1)
final.test$MSSubClass <- factor(final.test$MSSubClass)
final.test$GarageArea <- log(final.test$GarageArea + 1)
final.test$X2ndFlrSF <- log(final.test$X2ndFlrSF + 1)
final.test$TotalBsmtSF <- log(final.test$TotalBsmtSF + 1)
final.bstForest <- randomForest(SalePrice ~ ., mtry = 25, data = final.original, ntree = 500)
final.bstForest
final.yHat <- predict(final.bstForest, newdata = final.test)
sumamry(final.original)
summary(final.original)
summary(final.test)
summary(final.original)
summary(final.original)
levels(final.test$MSSubClass)
levels(final.original$MSSubClass)
levels(final.test$MSSubClass)
?e
PredictedSalePrice <- exp(yHat) + 1
IDColumns <- read.csv("train.csv")
IDColumns <- IDColumns$Id
PredictedSalePrice <- data.frame(ID = IDColumns, SalePrice = PredictedSalePrice)
head(PredictedSalePrice)
write.csv("KaggleSubmit1.csv")
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv")
totalData <- rbind(final.original, final.test)
totalData <- rbind(final.original[,-65], final.test)
for (f in 1:length(names(totalData))) {
levels(final.original[, f]) <- levels(totalData[, f])
}
final.bstForest <- randomForest(SalePrice ~ ., mtry = 25, data = final.original, ntree = 500)
final.yHat <- predict(final.bstForest, newdata = final.test)
for (name in colnames(final.original)) {
if (class(final.original[[name]]) == "factor")
print(name)
print(all(levels(final.original[[name]]) == levels(final.test[[name]])))
levels(final.test[[name]]) = levels(final.original[[name]])
print("-------------------")
}
final.yHat <- predict(final.bstForest, newdata = final.test)
PredictedSalePrice <- exp(final.yHat) + 1
PredictedSalePrice <- data.frame(ID = IDColumns, SalePrice = PredictedSalePrice)
PredictedSalePrice <- data.frame(ID = IDColumns[-1460], SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv")
head(PredictedSalePrice)
dim(PredictedSalePrice)
IDColumns <- read.csv("test.csv")
IDColumns <- IDColumns$Id
PredictedSalePrice <- data.frame(ID = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv")
dim(PredictedSalePrice)
head(PredictedSalePrice)
PredictedSalePrice <- exp(final.yHat) + 1
head(PredictedSalePrice)
head(IDColumns)
PredictedSalePrice <- data.frame(ID = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv")
head(PredictedSalePrice)
?write.csv
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE)
head(PredictedSalePrice)
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE)
head(PredictedSalePrice)
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE)
head(PredictedSalePrice)
PredictedSalePrice <- exp(final.yHat) + 1
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE)
head(PredictedSalePrice)
final.yHat <- predict(final.bstForest, newdata = final.test)
PredictedSalePrice <- exp(final.yHat) + 1
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE)
head(PredictedSalePrice)
PredictedSalePrice <- exp(final.yHat) + 1
PredictedSalePrice <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice, "KaggleSubmit1.csv", col.names = FALSE, row.names = FALSE)
head(PredictedSalePrice)
?stdev
?standarddeviation
??standard deviation
??standarddeviation
sd(final.original$SalePrice)
min(final.original$SalePrice)
max(final.original$SalePrice)
sd(exp(final.original$SalePrice) + 1)
sd(exp(final.original$SalePrice) - 1)
sd(exp(final.original$SalePrice) - 1) * 0.14
sd(exp(final.original$SalePrice) - 1) * 0.05
sd(exp(final.original$SalePrice) - 1) * 0.07
ridge.lasso.caret <- train(x.final, y.final, method =  'glmnet',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
x.final <- model.matrix(SalePrice ~ ., data = final.original)[ , -1]
y.final <- final.original$SalePrice
x.final.test <- model.matrix(SalePrice ~., data = final.test)[, -1]
y.final.test <- final.test$SalePrice
x.final.test <- model.matrix(data = final.test)[, -1]
x.final.test <- model.matrix(~., data = final.test)[, -1]
y.final.test <- final.test$SalePrice
#---Some Cross Validation---#
set.seed(0)
train_lasso_control <- trainControl(method = 'cv', number = 10)
tune.lasso.grid <- expand.grid(lambda = grid, alpha = seq(0,1, length = 100))
final.ridge.lasso.caret <- train(x.final, y.final, method =  'glmnet',
trControl = train_lasso_control, tuneGrid = tune.lasso.grid)
final.pred.lasso <- predict.train(ridge.lasso.caret, newdata = x.test)
PredictedSalePrice2 <- exp(final.pred.lasso) + 1
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice)
write.csv(x = PredictedSalePrice2, "KaggleSubmit2.csv", col.names = FALSE, row.names = FALSE)
head(PredictedSalePrice2)
PredictedSalePrice2 <- exp(final.pred.lasso) + 1
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2)
head(PredictedSalePrice2)
final.pred.lasso <- predict.train(ridge.lasso.caret, newdata = x.final.test)
dim(x.final)
dim(x.final.test)
x.final <- model.matrix(SalePrice ~ ., data = final.original)[ , -1]
y.final <- final.original$SalePrice
x.final.test <- model.matrix(~., data = final.test)[, -1]
final.pred.lasso <- predict.train(ridge.lasso.caret, newdata = x.final.test)
final.pred.lasso <- predict.train(ridge.lasso.caret, newx = x.final.test)
PredictedSalePrice2 <- exp(final.pred.lasso) + 1
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2)
dim(x.final.test)
final.pred.lasso <- predict.train(final.ridge.lasso.caret, newx = x.final.test)
PredictedSalePrice2 <- exp(final.pred.lasso) + 1
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2)
IDColumns <- IDColumns$Id
IDColumns <- read.csv("test.csv")
IDColumns <- IDColumns$Id
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2)
dim(IDColumns)
nrow(IDColumns)
length(IDColumns)
length(PredictedSalePrice2)
length(final.pred.lasso)
nrow(final.test)
x.final.test <- model.matrix(~., data = final.test)[, -1]
nrow(x.final.test)
final.pred.lasso <- predict.train(final.ridge.lasso.caret, newx = x.final.test)
nrow(final.pred.lasso)
length(final.pred.lasso)
PredictedSalePrice2 <- data.frame(Id = IDColumns, SalePrice = PredictedSalePrice2[-1460])
write.csv(x = PredictedSalePrice2, "KaggleSubmit2.csv", col.names = FALSE, row.names = FALSE)
length(final.pred.lasso)
sd(exp(final.original$SalePrice) - 1) * 0.12
summary(final.ridge.lasso.caret)
final.ridge.lasso.caret$bestTune
coef(final.ridge.lasso.caret)
coef(final.ridge.lasso.caret$modelInfo)
final.ridge.lasso.caret$modelInfo
final.ridge.lasso.caret$results
final.ridge.lasso.caret$perfNames
final.ridge.lasso.caret$finalModel$beta
final.ridge.lasso.caret$finalModel
final.ridge.lasso.caret$finalModel$beta$coef
final.ridge.lasso.caret$finalModel$beta
lin.model.lasso$beta
lin.model.lasso$beta$best
lin.model.lasso$beta
lin.model.lasso$beta[,1]
lin.model.lasso$beta[,54]
lin.model.lasso$beta[,100]
#######################
#####Tools for PCA#####
#######################
library(psych) #Library that contains helpful PCA functions, such as:
install.packages("psych")
#######################
#####Tools for PCA#####
#######################
library(psych) #Library that contains helpful PCA functions, such as:
principal() #Performs principal components analysis with optional rotation.
fa.parallel() #Creates scree plots with parallell analyses for choosing K.
factor.plot() #Visualizes the principal component loadings.
############################
#####Data for Example 1#####
############################
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
Harman23
Harman23.cor
####################
#####Choosing K#####
####################
fa.parallel(bodies, #The data in question.
n.obs = 305, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_bodies
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 8, #The number of PCs to extract.
rotate = "none")
pc_bodies
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_bodies
########################################
#####Visualizing & Interpreting PCA#####
########################################
factor.plot(pc_bodies,
labels = colnames(bodies)) #Add variable names to the plot.
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "varimax")
pc_bodies
########################################
#####Visualizing & Interpreting PCA#####
########################################
factor.plot(pc_bodies,
labels = colnames(bodies)) #Add variable names to the plot.
########################
#####Performing PCA#####
########################
pc_bodies = principal(bodies, #The data in question.
nfactors = 2, #The number of PCs to extract.
rotate = "none")
pc_bodies
########################################
#####Visualizing & Interpreting PCA#####
########################################
factor.plot(pc_bodies,
labels = colnames(bodies)) #Add variable names to the plot.
############################
#####Data for Example 2#####
############################
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
####################
#####Choosing K#####
####################
fa.parallel(iris_meas, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
####################
#####Choosing K#####
####################
fa.parallel(iris_meas, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
############################
#####Data for Example 2#####
############################
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
####################
#####Choosing K#####
####################
fa.parallel(iris_meas, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
########################
#####Performing PCA#####
########################
pc_iris = principal(iris_meas, #The data in question.
nfactors = 2,
rotate = "none") #The number of PCs to extract.
pc_iris
factor.plot(pc_iris,
labels = colnames(iris_meas)) #Add variable names to the plot.
################################
#####Viewing Projected Data#####
################################
plot(iris_meas) #Original data: 4 dimensions.
plot(pc_iris$scores) #Projected data: 2 dimensions.
############################
#####Data for Example 3#####
############################
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
factor.plot(pc_printer) #Add variable names to the plot.
plot(printer_data)
pairs(pc_printer$scores)
plot(printer_data)
pairs(pc_printer$scores)
### Preprocess with PCA with caret
library(caret)
ctrl <- trainControl(preProcOptions = list(thres = 0.90,
pcaComp = 3))
md = train(Species ~ ., data = iris,
method = 'glmnet',
preProc = 'pca',
family = 'multinomial',
trControl = ctrl)
### The predictors included in the final model
md$finalModel$xNames
### The predictors included in the final model
md$finalModel
### The predictors included in the final model
md$finalModel$xNames
md$finalModel$nulldev
md$finalModel$nobs
md$finalModel$jerr
