{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Importing Libraries #\n",
    "#######################\n",
    "\n",
    "#--Adding Data Types--#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#--Processing--#\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#--RandomForest--#\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#--Gradient Boosting--#\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#--Extreme Gradient Boosting--#\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#--Linear ElasticNet Regression--#\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "#--Pipeline For Stacking--#\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#--Error Metric--#\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#--Optimization--#\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "#--Visualizations--#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation For Tree's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Loading the Data #\n",
    "####################\n",
    "\n",
    "train_clean = pd.read_csv(\"train_clean.csv\")\n",
    "test_clean = pd.read_csv(\"test_clean.csv\")\n",
    "\n",
    "print(\"Training Dimensions: \", train_clean.shape)\n",
    "print(\"Testing Dimensions: \", test_clean.shape)\n",
    "\n",
    "######################\n",
    "# Getting Id Columns #\n",
    "######################\n",
    "colId = pd.read_csv(\"test.csv\")\n",
    "colId = colId.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Applying Transforms to Functions #\n",
    "####################################\n",
    "\n",
    "train_clean['SalePrice'] = train_clean['SalePrice'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['GarageArea'] = train_clean['GarageArea'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['X2ndFlrSF'] = train_clean['X2ndFlrSF'].apply(lambda x: np.log(x + 1))\n",
    "train_clean['TotalBsmtSF'] = train_clean['TotalBsmtSF'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "test_clean['GarageArea'] = test_clean['GarageArea'].apply(lambda x: np.log(x + 1))\n",
    "test_clean['X2ndFlrSF'] = test_clean['X2ndFlrSF'].apply(lambda x: np.log(x + 1))\n",
    "test_clean['TotalBsmtSF'] = test_clean['TotalBsmtSF'].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "\n",
    "\n",
    "for c in train_clean.columns:\n",
    "    if train_clean[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        train_clean[c] = le.fit_transform(train_clean[c].astype(str))\n",
    "for c in test_clean.columns:\n",
    "    if test_clean[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        test_clean[c] = le.fit_transform(test_clean[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################\n",
    "# # Altering the DataSet # (These changes are based on the feature importances by the models, to be commented out later)\n",
    "# ########################\n",
    "\n",
    "# # Random Forest Features #\n",
    "# ##########################\n",
    "# train_clean.drop(rf_lowest_features, axis = 1, inplace = True)\n",
    "# test_clean.drop(rf_lowest_features, axis = 1, inplace = True)\n",
    "\n",
    "# # Gradient Boosting Features # (These changes lowered both our Jupyter RMSE and our Kaggle Score)\n",
    "# ##############################\n",
    "\n",
    "# train_clean.drop(gb_removals, axis = 1, inplace = True)\n",
    "# test_clean.drop(gb_removals, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Splitting Data #  #(Only splitting the training data into two more sets called train_set, and test_set)\n",
    "##################\n",
    "\n",
    "train_set, test_set = train_test_split(train_clean, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "\n",
    "X_train = train_set.drop(\"SalePrice\", axis = 1)\n",
    "Y_train = train_set.SalePrice\n",
    "\n",
    "X_test = test_set.drop(\"SalePrice\", axis = 1)\n",
    "Y_test = test_set.SalePrice\n",
    "\n",
    "#########################################\n",
    "# The Full Original Training Set to Use #\n",
    "#########################################\n",
    "\n",
    "X_full_train = train_clean.drop(\"SalePrice\", axis = 1)\n",
    "Y_full_train = train_clean.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# RandomForest Model To See Best Features Split #\n",
    "#################################################\n",
    "mse = []\n",
    "for i in range(1,65):\n",
    "    randForest = RandomForestRegressor(n_estimators=1000, min_samples_leaf= 5, \n",
    "                                       max_features=i, oob_score = True, random_state=42, n_jobs=3)\n",
    "    randForest.fit(X_train, Y_train)\n",
    "    forestPredictions = randForest.predict(X_test)\n",
    "    mse.append(mean_squared_error(Y_test, forestPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Just to See The Index of the Lowest Tree #\n",
    "############################################\n",
    "lowest = 100000\n",
    "index = 100000\n",
    "for i,k in enumerate(mse):\n",
    "    if k < lowest:\n",
    "        lowest = k\n",
    "        index = i\n",
    "print(index, ':', lowest**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Running the Forest on The Whole Training #\n",
    "############################################\n",
    "\n",
    "randForest = RandomForestRegressor(n_estimators=10000, min_samples_leaf= 5, \n",
    "                                       max_features=48, oob_score = True, random_state=42, n_jobs=3)\n",
    "randForest.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Predicting The Kaggle DataSet with RandomForest #\n",
    "###################################################\n",
    "\n",
    "KagglePredictions = randForest.predict(test_clean)\n",
    "KagglePredictions = [np.exp(x) - 1 for x in KagglePredictions]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictions, \"Id\": colId}).to_csv(\"KaggleSubmitPythonForest.csv\", index = False)\n",
    "print(KagglePredictions[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Setting Up Gradient Boosting #\n",
    "################################\n",
    "\n",
    "def gradBoostCV(n_estimators, max_depth, max_features, min_samples_leaf):\n",
    "    val = cross_val_score(GradientBoostingRegressor(\n",
    "    n_estimators = int(n_estimators), max_depth = int(max_depth), max_features = int(max_features), min_samples_leaf = int(min_samples_leaf),\n",
    "        random_state = 42, learning_rate = 0.05\n",
    "    ),X_train, Y_train, scoring = 'neg_mean_squared_error', cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "gradBoostBaye = BayesianOptimization(gradBoostCV, {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (1,15),\n",
    "    \"max_features\": (1,63),\n",
    "    'min_samples_leaf': (5,10)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoostBaye.maximize(n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Results')\n",
    "print('Gradient Boosting: ', gradBoostBaye.res['max']['max_val'])\n",
    "print('Gradient Boosting: ', gradBoostBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# MSE of Running the GradBoost #\n",
    "################################\n",
    "\n",
    "testGradBoost = GradientBoostingRegressor(n_estimators=1368, max_depth=2, max_features=5, min_samples_leaf = 5,\n",
    "                                          random_state=42, learning_rate=0.05)\n",
    "testGradBoost.fit(X_train, Y_train)\n",
    "testGradBoostPredictions = testGradBoost.predict(X_test)\n",
    "mean_squared_error(Y_test, testGradBoostPredictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Running Gradient Boosting #\n",
    "#############################\n",
    "\n",
    "gradBoost = GradientBoostingRegressor(n_estimators=1368, max_depth=2, max_features=5,  min_samples_leaf = 5,\n",
    "                                      random_state=42, learning_rate=0.05)\n",
    "gradBoost.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Predicting The Kaggle DataSet with Gradient Boosting #\n",
    "########################################################\n",
    "\n",
    "KagglePredictionsGradBoost = gradBoost.predict(test_clean)\n",
    "KagglePredictionsGradBoost = [np.exp(x) - 1 for x in KagglePredictionsGradBoost]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsGradBoost, \"Id\": colId}).to_csv(\"KaggleSubmitPythonGradBoost.csv\", index = False)\n",
    "print(KagglePredictionsGradBoost[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Setting Up XG Boosting #\n",
    "###########################\n",
    "\n",
    "def xgBoostCV(n_estimators, max_depth, gamma, min_child_weight):\n",
    "    val = cross_val_score(XGBRegressor(n_estimators=int(n_estimators), max_depth=int(max_depth), \n",
    "                                      gamma = gamma, min_child_weight = min_child_weight, learning_rate = 0.05),\n",
    "                          X_train, Y_train, scoring = 'neg_mean_squared_error', \n",
    "                          cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "xgBoostBaye = BayesianOptimization(xgBoostCV, {\n",
    "    'n_estimators': (100, 10000),\n",
    "    'max_depth': (1,30),\n",
    "    \"gamma\": (0,50),\n",
    "    'min_child_weight': (1,50)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoostBaye.maximize(n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Results')\n",
    "print('XG Boosting: ', xgBoostBaye.res['max']['max_val'])\n",
    "print('XG Boosting: ', xgBoostBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# MSE of Running the XG Boost #\n",
    "###############################\n",
    "\n",
    "testXGBoost = XGBRegressor(n_estimators=2724, max_depth=30, gamma=0, min_child_weight = 4, learning_rate=0.05, nthread = 3)\n",
    "testXGBoost.fit(X_train, Y_train)\n",
    "testXGBoostPredictions = testXGBoost.predict(X_test)\n",
    "mean_squared_error(Y_test, testXGBoostPredictions) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Running XG Boosting #\n",
    "#######################\n",
    "\n",
    "XGBoost = XGBRegressor(n_estimators=2724, max_depth=30, gamma = 0, min_child_weight=4, learning_rate=0.05, nthread = 3)\n",
    "XGBoost.fit(X_full_train, Y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Predicting The Kaggle DataSet with XG Boosting #\n",
    "##################################################\n",
    "\n",
    "KagglePredictionsXGBoost = XGBoost.predict(test_clean)\n",
    "KagglePredictionsXGBoost = [np.exp(x) - 1 for x in KagglePredictionsXGBoost]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsXGBoost, \"Id\": colId}).to_csv(\"KaggleSubmitPythonXGBoost.csv\", index = False)\n",
    "print(KagglePredictionsXGBoost[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation For Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Loading the Data #\n",
    "####################\n",
    "\n",
    "train_clean_2 = pd.read_csv(\"train_clean.csv\")\n",
    "test_clean_2 = pd.read_csv(\"test_clean.csv\")\n",
    "\n",
    "print(\"Training Dimensions: \", train_clean_2.shape)\n",
    "print(\"Testing Dimensions: \", test_clean_2.shape)\n",
    "\n",
    "######################\n",
    "# Getting Id Columns #\n",
    "######################\n",
    "colId_2 = pd.read_csv(\"test.csv\")\n",
    "colId_2 = colId_2.Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Applying Transforms to Functions #\n",
    "####################################\n",
    "\n",
    "full_one_hot = pd.concat([train_clean_2, test_clean_2])\n",
    "full_one_hot['SalePrice'] = full_one_hot['SalePrice'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['GarageArea'] = full_one_hot['GarageArea'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['X2ndFlrSF'] = full_one_hot['X2ndFlrSF'].apply(lambda x: np.log(x+1))\n",
    "full_one_hot['TotalBsmtSF'] = full_one_hot['TotalBsmtSF'].apply(lambda x: np.log(x+1))\n",
    "\n",
    "####################\n",
    "# Random Deletions #\n",
    "####################\n",
    "full_one_hot.drop('GarageArea', axis = 1, inplace=True)\n",
    "full_one_hot.drop('GrLivArea', axis = 1, inplace=True)\n",
    "###########################\n",
    "# End of Random Deletions #\n",
    "###########################\n",
    "\n",
    "full_one_hot = pd.get_dummies(full_one_hot, drop_first=True, dummy_na=True)\n",
    "\n",
    "one_hot_train = full_one_hot[0:1460]\n",
    "one_hot_test = full_one_hot[1460:].drop('SalePrice', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Splitting Data #  #(Only splitting the training data into two more sets called train_set, and test_set)\n",
    "##################\n",
    "\n",
    "train_set_2, test_set_2 = train_test_split(one_hot_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"Train Shape: \", train_set_2.shape)\n",
    "print(\"Test Shape: \", test_set_2.shape)\n",
    "\n",
    "X_train_2 = train_set_2.drop(\"SalePrice\", axis = 1)\n",
    "Y_train_2 = train_set_2.SalePrice\n",
    "\n",
    "X_test_2 = test_set_2.drop(\"SalePrice\", axis = 1)\n",
    "Y_test_2 = test_set_2.SalePrice\n",
    "\n",
    "#########################################\n",
    "# The Full Original Training Set to Use #\n",
    "#########################################\n",
    "\n",
    "X_full_train_2 = one_hot_train.drop(\"SalePrice\", axis = 1)\n",
    "Y_full_train_2 = one_hot_train.SalePrice\n",
    "print(\"Full Shape: \", X_full_train_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LinRegCV(alpha, l1_ratio):\n",
    "    val = cross_val_score(make_pipeline(StandardScaler(), ElasticNet(alpha = alpha, l1_ratio = l1_ratio, random_state=42)),\n",
    "                         X_train_2, Y_train_2, scoring = 'neg_mean_squared_error', \n",
    "                          cv = 10, n_jobs = 3).mean()\n",
    "    return val\n",
    "\n",
    "LinRegBaye = BayesianOptimization(LinRegCV,{\n",
    "    'alpha': (0,1),\n",
    "    'l1_ratio': (0,1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinRegBaye.maximize(n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final Results')\n",
    "print('Linear Regression: ', LinRegBaye.res['max']['max_val'])\n",
    "print('Linear Regression: ', LinRegBaye.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# MSE of Running the Linear Regression #\n",
    "########################################\n",
    "\n",
    "testLinReg = make_pipeline(StandardScaler(), ElasticNet(alpha = 0.51496038449599246, l1_ratio = 0, random_state=42))\n",
    "testLinReg.fit(X_train_2, Y_train_2)\n",
    "testLinRegPredictions = testLinReg.predict(X_test_2)\n",
    "# print(testLinRegPredictions[:20])\n",
    "mean_squared_error(Y_test_2, testLinRegPredictions)**.5\n",
    "\n",
    "# testLinReg = Ridge(random_state=42, tol=0.000000001)\n",
    "# testLinReg.fit(X_train_2, Y_train_2)\n",
    "# testLinRegPredictions = testLinReg.predict(X_test_2)\n",
    "# print(testLinRegPredictions[:20])\n",
    "# print(mean_squared_error(Y_test_2, testLinRegPredictions)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Running Linear Regression #\n",
    "#############################\n",
    "\n",
    "LinReg = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.51496038449599246, l1_ratio = 0.0, random_state=42))\n",
    "LinReg.fit(X_full_train_2, Y_full_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Predicting The Kaggle DataSet with Linear Regression #\n",
    "########################################################\n",
    "\n",
    "KagglePredictionsLinReg = LinReg.predict(one_hot_test)\n",
    "KagglePredictionsLinReg = [np.exp(x) - 1 for x in KagglePredictionsLinReg]\n",
    "pd.DataFrame({\"SalePrice\":KagglePredictionsLinReg, \"Id\": colId_2}).to_csv(\"KaggleSubmitPythonLinReg.csv\", index = False)\n",
    "print(KagglePredictionsLinReg[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# The Importances of RandomForests #\n",
    "####################################\n",
    "\n",
    "rf_importances = randForest.feature_importances_\n",
    "rf_importances = pd.DataFrame({\"Features\": X_full_train.columns, \"Importances\": rf_importances})\n",
    "rf_importances = rf_importances.sort_values(by = \"Importances\", ascending=False)\n",
    "\n",
    "rf_lowest_features = list(rf_importances.Features.tail().values)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.xticks(rotation =90)\n",
    "# plt.ylim([0,0.04])\n",
    "\n",
    "barplot_ypos = np.arange(len(rf_importances.Features))\n",
    "plt.bar(barplot_ypos, rf_importances.Importances, align = 'center')\n",
    "plt.xticks(barplot_ypos, rf_importances.Features)\n",
    "plt.xlabel('Feature Name')\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Importance vs. Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The Importances of Gradient Boosting #\n",
    "########################################\n",
    "\n",
    "gb_importances = gradBoost.feature_importances_\n",
    "\n",
    "gb_importances = pd.DataFrame({\"Features\": X_full_train.columns, \"Importances\": gb_importances})\n",
    "gb_importances = gb_importances.sort_values(by = \"Importances\", ascending = True)\n",
    "\n",
    "# gb_lowest_features = list(gb_importances.Features.tail().values)\n",
    "# gb_removals = list(gb_importances.Features.tail()[-1:-3:-1])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 16)\n",
    "# plt.xticks(rotation =90)\n",
    "\n",
    "barplot_ypos = np.arange(len(gb_importances.Features))\n",
    "plt.barh(barplot_ypos, gb_importances.Importances, align = 'center')\n",
    "plt.yticks(barplot_ypos, gb_importances.Features)\n",
    "plt.ylabel('Feature Name')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Importance vs. Feature')\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig('GradBoost.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# The Importances of Gradient Boosting #\n",
    "########################################\n",
    "\n",
    "# XGBoost.plot_importance(XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blah = np.sum([KagglePredictions, KagglePredictionsGradBoost, KagglePredictionsXGBoost], axis = 0)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame({\"SalePrice\":blah, \"Id\": colId}).to_csv(\"KaggleSubmitPythonBlah.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlations = train_clean_2.corr()\n",
    "# plot correlation matrix\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "# ticks = np.arange(0,9,1)\n",
    "# ax.set_xticks(ticks)\n",
    "# ax.set_yticks(ticks)\n",
    "# plt.show()\n",
    "for i in correlations:\n",
    "    for j in range(len(correlations)):\n",
    "        x = abs(correlations[i][j])\n",
    "        if x >= 0.8 and x != 1:\n",
    "            print(i, j, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
